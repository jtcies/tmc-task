```{r}

library(tidyverse)
library(tidymodels)
library(lubridate)
library(factoextra)
library(xgboost)
library(mlr)

theme_set(theme_minimal())
test_data <- read_csv(here::here("data/test_data.csv"))

```

```{r}

colnames(test_data)
```

There are nine features, plus a key and our target. I'm thinking a tree-based model might be the most appropriate option.

What is the breakdown of our label? 

```{r}

test_data %>%
    count(target)

```
Some preprocessing.

```{r}

test_data <- test_data %>%
    mutate_at(vars(ideo, employ, target), as.factor) %>%
    mutate(
      age = year(as.period(interval(ymd(paste0(birth_year, "0101")), today())))
    )

```

About a 50/50 split between our two groups. I don't think we'll have to worry about class imbalance. I'll split up the data between a training, holdout, and test date. 

```{r}
test <- test_data %>%
    filter(is.na(target))

set.seed(2019)

holdout_ids <- test_data %>%
    filter(!is.na(target)) %>%
    sample_frac(.2) %>%
    pull(key)

train <- test_data %>%
    filter(!is.na(target), !key %in% holdout_ids)

holdout <- test_data %>%
    filter(!is.na(target), key %in% holdout_ids)

```

## Exploring predictors

Start with age

```{r}

train %>% 
    ggplot(aes(age)) +
        geom_histogram(binwidth = 2)

```

Mostly boomers. My guess is that this data is of likley voters rather than whole population since it is so skewed towards individuals above 50. 


```{r}

train %>% 
    select(gender, ideo, race_4, edu, employ, party) %>% 
    map(table, useNA = "always") 

```

A small amount of missingness we'll have to deal with. Mostly white, college educated, but a pretty even split between Democrats and Repbulicans. I wonder if missing is true missing or if that means independent. Is there any correlation between missing variables?

```{r}

train %>% 
    filter(is.na(party)) %>% 
    count(gender, race_4, edu)

```

I think for `race` and `edu` we should fill in missing with the mode and for party we can impute from the other variables. 

I wonder what that labels in `ideo` stand for?  

```{r}
train %>% 
    group_by(party) %>% 
    count(ideo) %>% 
    mutate(pct = n / sum(n)) %>% 
    select(-n) %>%
    spread(ideo, pct)
```

Looks like 1 is progressive, -1 is conservative. Missing is party is spread out evenly among ideology, which makes me think it is true missing. I would assume party based on ideology, but we have people with 0 ideology. In that case, we can use a logisitc regression to imput it from the other variables. 


```{r}

party_mod <- glm(
  factor(party) ~ ., 
  data = train %>% select(-target, -key, -birth_year) %>% filter(!is.na(party)),
  family = binomial(link = "logit")
)

summary(party_mod)

```

Looks like ideology and race are the only significant factors. 

```{r}

train %>% 
  group_by(ideo, race_4) %>% 
  count(party) %>% 
  mutate(pct = n / sum(n)) %>% 
  ggplot(aes(party, pct)) +
    geom_col() +
    facet_grid(race_4 ~ ideo)

```

Check how the target relates to party and ideology. 

```{r}
train %>%
    count(ideo, target) %>%
    spread(target, n)
```

1 is support for the policy (which restricts access to reproductive rights), 0 is oppose.

Let's look at how categorical variables relate to the target. 

```{r}

var_counts <- function(predictor) {

    pred <- rlang::sym(predictor)

    train %>%
        mutate(var = as.character(fct_explicit_na(!! pred))) %>% 
        group_by(var) %>%
        count(target) %>%
        ungroup() %>% 
        transmute(
          pred = predictor,
          var = var,
          target = target,
          pct = n /  sum(n)
        )
    
}

vars <- c("gender", "race_4", "edu", "employ", "party", "ideo")

map_dfr(vars, var_counts) %>% 
  ggplot(aes(var, pct, fill = target)) +
    geom_col(position = "dodge") +
    facet_wrap(~pred, scales = "free_x")

```

Education and employ do not have strong connections with stance on policy. Women are less likely to support the policy even though a plurarily of women do still support it. Party may be a stronger predictor than even ideology, especially among republicans. Respondenents who are Black or Latinx are less likley to support the policy. 

Let's take a look at some of the numerical variables. First just the distributions.

```{r}


num_vars <- train %>% 
  select(activist_pct, income_pct, age, target) %>% 
  gather(var, val, 1:3) 

num_vars %>% 
  ggplot(aes(val)) +
    geom_histogram(bins = 20) +
    facet_wrap(~var, scales = "free_x")

```

Likelihood of being involed in activism looks like a uniform distribution. Age as we saw was skewed toward older folks, and income is a percentile that has already been normalized within our sample. I wonder how the 'activism' question was asked given that it's a percent.

Let's see how they relate to target?

```{r}

num_vars %>% 
  ggplot(aes(val, fill = target)) +
    geom_histogram(bins = 20, position = "fill") +
    facet_wrap(~var, scales = "free_x")

```

It looks like older respondents are more likely to support the policy. Those who are wealthier are also more likely to support the policy, but this only shows up at the extreme ends of the distribution. Activist percent doesn't have a connection with policy support. I wonder if some feature engineering would help out with this? 

Let's see how current values relate to policy support? 

```{r}

train %>% 
  group_by(ideo, target) %>% 
  summarise(med_activist_pct = mean(activist_pct)) %>% 
  spread(target, med_activist_pct)

```

Interestingly, it looks like progressives who support the policy policy across ideological lines are more likley to be activist than those who oppose the policy. Perhaps there is some signal that. 

Let's see if some modeling can help us parse out any other signals? 

```{r}

u2 <- runif(nrow(train))

linear_mod <- glm(
  target ~ ., 
  data = train %>% 
    mutate(
      race_4 = fct_relevel(race_4, "White"),
      ideo = fct_relevel(ideo, "0")
    ) %>% 
    select(-key, -birth_year),
  family = binomial(link = "logit")
)

linear_mod %>% 
  tidy() %>% 
  filter(term != "(Intercept)") %>% 
  mutate(
    term = fct_reorder(term, estimate),
    upper = estimate + 2 * std.error,
    lower = estimate - 2 * std.error
  ) %>% 
  ggplot(aes(term, estimate, ymin = lower, ymax = upper)) +
    geom_point() +
    geom_linerange() +
    coord_flip() +
    geom_hline(yintercept = 0, linetype = 3) +
    labs(
      title = "Coefficient plot for logisitc regression of policy support"
    )

```

Unsuprisingly, the strongest signals seems to be party and ideology. Gender too has a statistically significant effect.

This also tells me that conservatives feel more strongly about this policy than progressives do. Conservative ideology has a much stronger effect on policy support than progressive ideology does on policy opposition.

Fill in the missing variables. 

```{r}
fill_missing <- function(data) {

    data %>%
        replace_na(list(race_4 = "White", employ = 0, edu = "college")) %>%
        mutate(
            party_pred = case_when(
                is.na(party) ~ predict(party_mod, ., type = "response"),
                party == "Dem" ~ 0,
                party == "Rep" ~ 1
            ),
            party = case_when(
                is.na(party) & party_pred >= 0.5 ~ "Dem",
                is.na(party) & party_pred < 0.5 ~ "Rep",
                TRUE ~ party
            )
       ) %>%
       select(-party_pred)

}

train <- fill_missing(train)
holdout <- fill_missing(holdout)
test <- fill_missing(test)

```
## Modeling

```{r}

create_matrix <- function(data) {

    target <- as.numeric(as.character(data$target))

    mat <- model.matrix(
        ~ . + 0,
        data = data %>%
            select(-key, -target, -birth_year)
    )

    xgb.DMatrix(data = mat, label = target)

}


train_matrix <- create_matrix(train)


```

Finding the ideal parameters.

```{r}

# conver this to map that returns a data frame with the paramters

best_params <- list()
best_seed <- 1111
best_auc = 0
best_round = 0

for (i in 1:100) {
  
  
  params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    max_depth = sample(3, 10, 1),
    eta = sample(seq(0.01, 3, by = 0.01), 1),
    min_child_weight = sample(1:40, 1),
    subsample = sample(seq(0.5, 0.8, by = 0.1), 1),
    colsample_bytree = sample(seq(0.5, 0.9, by = 0.1), 1)
  )
  
  
  seed <- set.seed(sample.int(10000, 1))
  
  xgb_cv <- xgb.cv(
    data = train_matrix,
    params = params,
    nfold = 6,
    nrounds = 1000,
    early_stopping_rounds = 8
  )
  
  best_iter_round <- xgb_cv$best_iteration 
  
  best_round_auc <- data.frame(xgb_cv$evaluation_log) %>% 
    dplyr::slice(best_iter_round) %>% 
    pull(test_auc_mean)
  
  if (best_round_auc > best_auc) {
    
    best_auc <- best_round_auc
    best_round <- best_iter_round
    best_seed <- seed
    best_param <- params
    
  }
  
}


rounds <- best_round
set.seed(best_seed)

fit <- xgb.train(
  data = train_matrix,
  params = best_param,
  nrounds = rounds,
)


```

