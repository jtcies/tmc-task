```{r}

library(tidyverse)
library(tidymodels)
library(lubridate)
library(xgboost)
library(plotROC)

theme_set(theme_minimal())
test_data <- read_csv(here::here("data/test_data.csv"))

```

```{r}

colnames(test_data)
```

There are nine features, plus a key and our target. I'm thinking a tree-based model might be the most appropriate option.

What is the breakdown of our label? 

```{r}

test_data %>%
    count(target)

```
Some preprocessing.

```{r}

test_data <- test_data %>%
    mutate_at(vars(ideo, employ, target), as.factor) %>%
    mutate(
      age = year(as.period(interval(ymd(paste0(birth_year, "0101")), today())))
    )

```

About a 50/50 split between our two groups. I don't think we'll have to worry about class imbalance. I'll split up the data between a training, holdout, and test date. 

```{r}
test <- test_data %>%
    filter(is.na(target))

set.seed(2019)

holdout_ids <- test_data %>%
    filter(!is.na(target)) %>%
    sample_frac(.2) %>%
    pull(key)

train <- test_data %>%
    filter(!is.na(target), !key %in% holdout_ids)

holdout <- test_data %>%
    filter(!is.na(target), key %in% holdout_ids)

```

## Exploring predictors

Start with age

```{r}

train %>% 
    ggplot(aes(age, color = target)) +
        geom_density(size = 2)

```

Mostly boomers. My guess is that this data is of likley voters rather than whole population since it is so skewed towards individuals above 50. Older voters are slightly more likley to support the policy.

```{r}

train %>% 
    select(gender, ideo, race_4, edu, employ, party) %>% 
    map(table, useNA = "always") 

```

A small amount of missingness we'll have to deal with. Mostly white, college educated, but a pretty even split between Democrats and Repbulicans. I wonder if missing is true missing or if that means independent. Is there any correlation between missing variables?

```{r}

train %>% 
    filter(is.na(party)) %>% 
    count(gender, race_4, edu)

```

I think for `race` and `edu` we should fill in missing with the mode.

I wonder what that labels in `ideo` stand for?  

```{r}
train %>% 
    group_by(party) %>% 
    count(ideo) %>% 
    mutate(pct = n / sum(n)) %>% 
    select(-n) %>%
    spread(ideo, pct)
```

Looks like 1 is progressive, -1 is conservative. Those missing `party` are spread out evenly among ideology, which makes me think it is true missing. I would assume party based on ideology, but we have people with 0 ideology. 

I initally tried imputing party from the other variables, but the model performs better when just making the missing explicit. Presumably there is information contained in the missingness.

Check how the target relates to party and ideology. 

```{r}
train %>%
    count(ideo, target) %>%
    spread(target, n)
```

1 is support for the policy (which restricts access to reproductive rights), 0 is oppose.

Let's look at how categorical variables relate to the target. 

```{r}

var_counts <- function(predictor) {

    pred <- rlang::sym(predictor)

    train %>%
        mutate(var = as.character(fct_explicit_na(!! pred))) %>% 
        group_by(var) %>%
        count(target) %>%
        ungroup() %>% 
        transmute(
          pred = predictor,
          var = var,
          target = target,
          pct = n /  sum(n)
        )
    
}

vars <- c("gender", "race_4", "edu", "employ", "party", "ideo")

map_dfr(vars, var_counts) %>% 
  ggplot(aes(var, pct, fill = target)) +
    geom_col(position = "dodge") +
    facet_wrap(~pred, scales = "free_x")

```

Education and employment do not have strong connections with stance on policy. Women are less likely to support the policy even though a plurality of women do still support it. Party may be a stronger predictor than even ideology, especially among republicans. Respondenents who are Black or Latinx are less likley to support the policy. 

Let's take a look at some of the numerical variables. First just the distributions.

```{r}

num_vars <- train %>% 
  select(activist_pct, income_pct, age, target) %>% 
  gather(var, val, 1:3) 

num_vars %>% 
  ggplot(aes(val)) +
    geom_histogram(bins = 20) +
    facet_wrap(~var, scales = "free_x")

```

Likelihood of being involed in activism looks like a uniform distribution. Age as we saw was skewed toward older folks, and income is a percentile that has already been normalized within our sample. I wonder how the 'activism' question was asked given that it's a percent.

Let's see how they relate to target?

```{r}

num_vars %>% 
  ggplot(aes(val, color = target)) +
    geom_density(size = 2) +
    facet_wrap(~var, scales = "free")

```

It looks like older respondents are more likely to support the policy. Those who are wealthier are also more likely to support the policy, but this only shows up at the extreme ends of the distribution. Activist percent doesn't have a connection with policy support. I wonder if some feature engineering would help out with this? 

Let's see how current values relate to policy support? 

```{r}

train %>% 
  group_by(ideo, target) %>% 
  summarise(med_activist_pct = mean(activist_pct)) %>% 
  spread(target, med_activist_pct)

```

Interestingly, it looks like, across ideological lines, those who support the policy are more likley to be activist than those opposed to it. Perhaps the policy is controversial and those who support it think of themselves as activists or change agents?

Let's see if some modeling can help us parse out any other signals? 

```{r}


linear_mod <- glm(
  target ~ ., 
  data = train %>% 
    mutate(
      race_4 = fct_relevel(race_4, "White"),
      ideo = fct_relevel(ideo, "0")
    ) %>% 
    select(-key, -birth_year),
  family = binomial(link = "logit")
)

linear_mod %>% 
  tidy() %>% 
  filter(term != "(Intercept)") %>% 
  mutate(
    term = fct_reorder(term, estimate),
    upper = estimate + 2 * std.error,
    lower = estimate - 2 * std.error
  ) %>% 
  ggplot(aes(term, estimate, ymin = lower, ymax = upper)) +
    geom_point() +
    geom_linerange() +
    coord_flip() +
    geom_hline(yintercept = 0, linetype = 3) +
    labs(
      title = "Coefficient plot for logisitc regression of policy support"
    )

```

Unsuprisingly, the strongest signals seems to be party and ideology. Gender too has a statistically significant effect.

This also tells me that conservatives feel more strongly about this policy than progressives do. Conservative ideology has a much stronger effect on policy support than progressive ideology does on policy opposition.

One idea for engineering a feature is creating a retired flag. It may be that those who are unemployed at an older age have different views that those who are unemployed involuntarily. 

```{r}

train %>% 
  mutate(retired = if_else(age >= 65 & employ == 0, 1, 0)) %>% 
  filter(employ == 0) %>% 
  ggplot(aes(age, target, color = factor(retired))) +
    geom_jitter()

```

Fill in the missing variables. 

```{r}

fill_missing <- function(data) {

    data %>%
        replace_na(list(race_4 = "White", employ = 0, edu = "college")) %>%
        mutate(
            party = fct_explicit_na(party),
            retired = if_else(age >= 65 & employ == 0, 1, 0),
            prog_activist = if_else(activist_pct > 0.5 & ideo == 1, 1, 0)
       ) 

}

train <- fill_missing(train)
holdout <- fill_missing(holdout)
test <- fill_missing(test)

```

## Modeling

```{r}

create_matrix <- function(data) {

    target <- as.numeric(as.character(data$target))

    mat <- model.matrix(
        ~ . + 0,
        data = data %>%
            select(-key, -target, -birth_year)
    )

    xgb.DMatrix(data = mat, label = target)

}


train_matrix <- create_matrix(train)
holdout_matrix <- create_matrix(holdout)


```

Finding the ideal parameters.

```{r}
search_params <- function(ntimes) {
  
  results <- tibble()
  
  for(i in 1:ntimes) {
    
    seed <- sample.int(10000, 1)
    
    set.seed(seed)
    
    params <- list(
      objective = "binary:logistic",
      eval_metric = "auc",
      max_depth = sample(3:15, 1),
      eta = sample(seq(0.01, 0.3, by = 0.01), 1),
      min_child_weight = sample(1:20, 1),
      subsample = sample(seq(0.5, 0.8, by = 0.1), 1),
      colsample_bytree = sample(seq(0.5, 0.9, by = 0.1), 1)
    )
    
    xgb_cv <- xgb.cv(
      data = train_matrix,
      params = params,
      nfold = 6,
      nrounds = 1000,
      early_stopping_rounds = 10
    )
    
    log <- as_tibble(xgb_cv$evaluation_log)
    
    param_df <- tibble(
      max_depth = params$max_depth, 
      eta = params$eta, 
      min_child_weight = params$min_child_weight,
      subsample = params$subsample,
      colsample_bytree = params$colsample_bytree,
      seed = seed
    )
    
    this_result <- cbind(log, param_df)
    
    results <- bind_rows(results, this_result)
  
  }
  results 
  
}

param_results <- search_params(200) %>% 
  mutate(round = cumsum(iter == 1))

# find the best iteration from each round

best_iter <- param_results %>% 
  group_by(round) %>% 
  filter(test_auc_mean == max(test_auc_mean)) %>% 
  distinct(round, .keep_all = TRUE) %>% 
  ungroup()

best_iter %>% 
  ggplot(aes(test_auc_mean)) +
    geom_histogram(binwidth = 0.001)

# plot each param against test_auc

plot_params <- function(col) {
  
  col <- rlang::sym(col)
  
  ggplot(best_iter, aes(!! col, test_auc_mean)) +
    geom_point() +
    geom_smooth()
  
}

c("max_depth", "eta", "subsample", "min_child_weight", "colsample_bytree") %>% 
  map(plot_params)

best_params <- best_iter %>% 
  filter(test_auc_mean == max(test_auc_mean)) %>% 
  dplyr::slice(1)

final_params <- list(
  max_depth = best_params$max_depth,
  min_child_weight = best_params$min_child_weight,
  eta = best_params$eta,
  subsample = best_params$subsample,
  colsample_bytree = best_params$colsample_bytree
)
```

Fit the model with those best parameters.

```{r}
rounds <- best_params %>% 
  pull(iter)
  
best_seed <- best_params %>% 
  pull(seed)

set.seed(best_seed)

fit <- xgb.train(train_matrix, params = final_params, nrounds = rounds)

```

Evaluate on the holdout set.

```{r}

train_pred <- tibble(
  key = train$key,
  target = as.numeric(as.character(train$target)), 
  pred = predict(fit, train_matrix),
  type = "train"
)

holdout_pred <- tibble(
  key = holdout$key,
  target = as.numeric(as.character(holdout$target)), 
  pred = predict(fit, holdout_matrix),
  type = "holdout"
)

predictions <- bind_rows(train_pred, holdout_pred) %>% 
  mutate(type = fct_relevel(type, "train")) 

write_csv(predictions, here::here("output/train_predictions.csv"))

roc_plot <- predictions %>% 
  ggplot(aes(d = target, m = pred, color = type)) +
    geom_roc() +
    geom_rocci()

calc_auc(roc_plot)

roc_plot

```

```{r}

predictions %>% 
  ggplot(aes(pred, color = factor(target))) +
    geom_density(size = 2) +
    facet_wrap(~type)

```

Calcuate F1 score

```{r}

calc_f1 <- function(cutoff) {
  
  predictions %>% 
    mutate(
      cutoff = cutoff,
      label = if_else(pred > cutoff, 1, 0),
      tp = if_else(target == 1 & label == 1, 1, 0),
      tn = if_else(target == 0 & label == 0, 1, 0),
      fp = if_else(target == 0 & label == 1, 1, 0),
      fn = if_else(target == 1 & label == 0, 1, 0)
    ) %>% 
    group_by(cutoff, type) %>% 
    summarise(
      prec = sum(tp) / (sum(tp) + sum(fp)),
      recall = sum(tp) / (sum(tp) + sum(fn))
    ) %>% 
    mutate(f1 = 2 * ((prec *  recall) / (prec + recall))) %>% 
    ungroup()
  
}

scores <- seq(0.1, 0.9, by = 0.1) %>% 
  map_dfr(calc_f1) 

scores %>% 
  arrange(type)

```

Somewhat suprisingly, scores on the holdout are slightly better than the training set. However, training set used cross-validation to determine model parameters and holdout is a pretty small sample, so this is probably ok. 

0.5 is the best cutoff score for all metrics. 

```{r}

xgb.importance(model = fit) %>%
  xgb.ggplot.importance()
  
  mutate(Feature = fct_reorder(Feature, Gain)) %>% 
  ggplot(aes(Feature, Gain)) +
    geom_col() +
    coord_flip()

```

## Apply predictions to test set

```{r}

test_matrix <- create_matrix(test)

test_pred <- tibble(
  key = test$key,
  pred = predict(fit, test_matrix)
) 

write_csv(test_pred, here::here("output/test_predictions.csv"))

```

